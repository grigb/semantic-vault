# docker-compose.yml
services:
  # -------------------------------------------
  # Embedding Proxy: LocalAI
  # -------------------------------------------
  embedding-proxy:
    build:
      context: ../embedding-proxy
      dockerfile: Dockerfile
    container_name: semantic-vault-embedding-proxy
    ports:
      - "9010:8080"
    networks:
      - ai_network

  # -------------------------------------------
  # LLM Host: AnythingLLM
  # -------------------------------------------
  anythingllm:
    image: mintplexlabs/anythingllm:latest # Use the official image
    container_name: semantic-vault-anythingllm
    depends_on:
      - graphiti # Make sure Graphiti starts first
    ports:
      - "9002:3001" # Changed host port to uncommon 9002 for AnythingLLM UI
    volumes:
      - ./anythingllm-storage:/app/server/storage # Persist AnythingLLM data (configs, vector cache, etc.)
      # - ./anythingllm-collector:/app/collector/storage # Persist collector data if used - Commented out to troubleshoot startup error
    environment:
      # --- Core Settings ---
      # Enables authentication. Set to true for production/shared use.
      # If true, use ANYTHINGLLM_INITIAL_ADMIN_KEY on first run.
      AUTH_MODE: "false" # Set to "true" for production/auth
      ANYTHINGLLM_INITIAL_ADMIN_KEY: ${ANYTHINGLLM_INITIAL_ADMIN_KEY:-} # Seed key if AUTH_MODE=true

      # --- Memory Configuration (Using Graphiti via MCP) ---
      # Tells AnythingLLM to use the Memory Connector Plugin
      MEMORY_BACKEND: "memory_connector_plugin"
      # URL of the Graphiti MCP endpoint inside the Docker network
      MCP_API_URL: "http://graphiti:8080/api/v1/memory"
      # API Key needed to talk to Graphiti (must match Graphiti's key)
      MCP_API_KEY: "${GRAPHITI_API_KEY}"
      # Optional: Define a default namespace within Graphiti for AnythingLLM's memories
      # MCP_NAMESPACE: "anythingllm_default"
    restart: unless-stopped
    networks:
      - ai_network

  # -------------------------------------------
  # Shared Memory Store: Graphiti
  # -------------------------------------------
  graphiti:
    build:
      context: ../graphiti
      dockerfile: Dockerfile
    container_name: semantic-vault-graphiti
    depends_on:
      - qdrant # Make sure the vector DB starts first
      - neo4j  # Make sure the graph DB starts first
    ports:
      - "9003:9083" # Changed host port to uncommon 9003 for Graphiti API (container now uses 9083)
    volumes:
      - /Users/grig/semantic-vault/graphiti/tests:/app/tests
    environment:
      # --- Core Settings ---
      GRAPHITI_API_KEY: "${GRAPHITI_API_KEY?GRAPHITI_API_KEY must be set in .env}"
      OPENAI_API_KEY: "${OPENAI_API_KEY?OPENAI_API_KEY must be set in .env}"
      OPENAI_API_BASE: "${OPENAI_API_BASE}"

      # --- Vector Store Connection (Qdrant) ---
      # Specify which vector store backend to use
      VECTOR_STORE: "qdrant"
      # Qdrant connection details (using service name within Docker network)
      QDRANT_HOST: "qdrant"
      QDRANT_PORT: "6333"
      # QDRANT_API_KEY: ${QDRANT_API_KEY:-} # Uncomment if Qdrant needs an API key

      # --- Graph Database Connection (Neo4j) ---
      # Even when Qdrant is the primary vector store, Graphiti uses Neo4j for graph memory.
      NEO4J_URI: "bolt://neo4j:7687"
      NEO4J_USER: "neo4j"
      NEO4J_PASSWORD: "${NEO4J_PASSWORD?NEO4J_PASSWORD must be set in .env}"

      # --- Graphiti Internal Storage (Optional: Use Postgres instead of SQLite) ---
      # DB_TYPE: "postgres"
      # DB_HOST: "postgres_db"
      # DB_PORT: "5432"
      # DB_USER: "${POSTGRES_USER}"
      # DB_PASSWORD: "${POSTGRES_PASSWORD}"
      # DB_NAME: "${POSTGRES_DB}"

    restart: unless-stopped
    networks:
      - ai_network

  # -------------------------------------------
  # Vector Database: Qdrant
  # -------------------------------------------
  qdrant:
    image: qdrant/qdrant:latest
    container_name: semantic-vault-qdrant
    ports:
      - "9004:6333" # Changed host port to uncommon 9004 for Qdrant HTTP
      - "9005:6334" # Changed host port to uncommon 9005 for Qdrant gRPC
    volumes:
      - qdrant-storage:/qdrant/storage # Persist Qdrant data (collections, vectors)
    # environment: # Uncomment if you need API key or specific Qdrant configs
    #   QDRANT__SERVICE__API_KEY: ${QDRANT_API_KEY:-}
    restart: unless-stopped
    networks:
      - ai_network

  # -------------------------------------------
  # Graph Database: Neo4j
  # -------------------------------------------
  neo4j:
    image: neo4j:5.18
    container_name: semantic-vault-neo4j
    ports:
      - "9006:7474" # Changed host port to uncommon 9006 for Neo4j Browser
      - "7687:7687" # Keep internal port as 7687 for Docker network resolution
    volumes:
      - ~/.ollama/models:/root/.ollama/models # Mount Ollama model cache for sharing across containers
      - neo4j-data:/data
    environment:
      # Sets the initial password for the 'neo4j' user.
      NEO4J_AUTH: neo4j/${NEO4J_PASSWORD?NEO4J_PASSWORD must be set in .env}
      NEO4J_PLUGINS: 'graph-data-science,apoc'
    restart: unless-stopped
    networks:
      ai_network:
        aliases:
          - neo4j
          - graphdb

# -------------------------------------------
# Volumes (for data persistence)
# -------------------------------------------
volumes:
  anythingllm-storage:
  anythingllm-collector:
  graphiti-data:
  qdrant-storage:
  neo4j-data: # Added volume for Neo4j persistence

# -------------------------------------------
# Network (allows containers to talk via service names)
# -------------------------------------------
networks:
  ai_network:
    driver: bridge
